{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 데이터 읽어오기 (변수 5개 (Np, Vp, Bz, Bt, Bl)만 남긴 combine)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "path='C:\\Projects\\keras_talk\\comp_model_data\\\\'\n",
    "\n",
    "combine_data = pd.read_csv(path + 'combine_interpolation_an.csv', engine='python') \n",
    "\n",
    "kp_data = pd.read_csv(path + 'kp_data.csv') # 지자기교란 지수 파일에서 index부분 없애고 kp_data로 이름 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, result_data, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(0, len(signal_data)-12, 12):\n",
    "        dataX.append(signal_data[i:(i+look_back)]) # look_back = 24 단위로 6시간씩 끊어짐 - [0:24],[24:48],...\n",
    "    for j in range(0, len(result_data)):\n",
    "        for k in range(0, 8):\n",
    "            dataY.append(result_data[j][k]) # kp지수 데이터의 하나 하나를 뽑아내어 6시간의 combine 데이터와 대응시킴\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "signal_data = combine_data.values\n",
    "result_data = kp_data.values\n",
    "\n",
    "x_train = signal_data[0:385728] # 2920*12*8 + 2928*12*3 (00,04,08년 윤년) = 385728 (99~09년까지의 데이터를 train으로)\n",
    "x_val = signal_data[385728:455808] # 2920*12*2=70080 (10~11년까지의 데이터를 val으로)\n",
    "x_test = signal_data[455808:] # 2920*12*1 + 2928*12*1 =70080 (12년 윤년) (12~13년까지의 데이터를 test으로)\n",
    "\n",
    "y_train = result_data[0:4018] # 99~09년도\n",
    "y_val = result_data[4018:4748] # 10~11년도\n",
    "y_test = result_data[4748:] # 12~13년도\n",
    "\n",
    "# 데이터셋 생성\n",
    "\n",
    "x_train, y_train = create_dataset(x_train, y_train, look_back)\n",
    "x_val, y_val = create_dataset(x_val, y_val, look_back)\n",
    "x_test, y_test = create_dataset(x_test, y_test, look_back)\n",
    "\n",
    "y_train = y_train[1:]\n",
    "y_val = y_val[1:]\n",
    "y_test = y_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 (RNN은 3차원으로 처리를 해야 한다고 함)\n",
    "# 변수 5개 (Np, Vp, Bz, Bt, Bl)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 5) # (number, timestep, feature)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 5)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 (어떤 값을 리스트 내에 0,1로 표시하는 벡터로 표현하도록 함, 예를 들어 3인 경우엔 [0,0,0,1,0,0,0,0,0,0]으로 표현됨)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "size = 10\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=size)\n",
    "y_val = to_categorical(y_val, num_classes=size)\n",
    "y_test = to_categorical(y_test, num_classes=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 16072 input samples and 32143 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-f5a009bb3475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# 4. 모델 학습시키기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    242\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 16072 input samples and 32143 target samples."
     ]
    }
   ],
   "source": [
    "# 2. 모델 구성하기\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "Optimizer = Adam(lr = 0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(48,input_shape=(24,5)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=100, batch_size=50, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 평가하기\n",
    "\n",
    "trainScore = model.evaluate(x_train, y_train, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "valScore = model.evaluate(x_val, y_val, verbose=0)\n",
    "model.reset_states()\n",
    "print('Validataion Score: ', valScore)\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. 모델 사용하기\n",
    "\n",
    "path='C:\\Projects\\keras_talk\\comp_model_data\\\\'\n",
    "\n",
    "# 룩백 12일때는 학습 데이터가 한 셀에 3시간치만 있게 되는데 \n",
    "# 룩백 24일때는 학습 데이터가 한 셀에 6시간치가 있어서 그것에 맞게 문제 데이터를 create_dataset으로 한 번 더 처리\n",
    "# 반대로 룩백 12일때는 그게 기본 설정이라 문제 데이터를 굳이 처리할 필요가 없었던 것\n",
    "\n",
    "def create_dataset(signal_data,look_back):\n",
    "    dataX = []\n",
    "    for i in range(0, len(signal_data)-12, 12): # look_back = 24 단위로 6시간씩 끊어짐\n",
    "        dataX.append(signal_data[i:(i+look_back)]) # [0:24],[24:48],..\n",
    "    return np.array(dataX)\n",
    "\n",
    "look_back = 24\n",
    "\n",
    "# 룩백이 24, 즉 6시간 뒤부터 보는 거라서 원래 problem_interpolation1 파일의 맨 앞에 전년도 마지막 3시간 데이터를 엑셀로 직접 덧붙임\n",
    "# 그렇게 problem_interpolation2를 만듦\n",
    "\n",
    "xhat = pd.read_csv(path + 'problem_interpolation2.csv', engine='python')\n",
    "\n",
    "input_data = xhat.values\n",
    "\n",
    "input_data = create_dataset(input_data, look_back)\n",
    "\n",
    "#input_data = input_data.reshape(input_data.shape[0], input_data.shape[1], 1)\n",
    "\n",
    "#input_data = input_data.reshape(1460, 12, 5) - 이 두 줄이 필요한지 안한지는 실행해봐야 알 것 같음.\n",
    "\n",
    "forecast_result = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예측한 kp지수를 엑셀로 다시 정리\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(0,2920):\n",
    "    result.append(np.argmax(forecast_result[i]))\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for i in range(0, len(result), 8):\n",
    "    final_result.append(result[i:i+8])\n",
    "\n",
    "kp_0h = []\n",
    "kp_3h = []\n",
    "kp_6h = []\n",
    "kp_9h = []\n",
    "kp_12h = []\n",
    "kp_15h = []\n",
    "kp_18h = []\n",
    "kp_21h = []\n",
    "\n",
    "for i in range(0, len(final_result)):\n",
    "    kp_0h.append(final_result[i][0])\n",
    "    kp_3h.append(final_result[i][1])\n",
    "    kp_6h.append(final_result[i][2])\n",
    "    kp_9h.append(final_result[i][3])\n",
    "    kp_12h.append(final_result[i][4])\n",
    "    kp_15h.append(final_result[i][5])\n",
    "    kp_18h.append(final_result[i][6])\n",
    "    kp_21h.append(final_result[i][7])\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "raw_data = OrderedDict()\n",
    "raw_data['kp_0h'] = kp_0h\n",
    "raw_data['kp_3h'] = kp_3h\n",
    "raw_data['kp_6h'] = kp_6h\n",
    "raw_data['kp_9h'] = kp_9h\n",
    "raw_data['kp_12h'] = kp_12h\n",
    "raw_data['kp_15h'] = kp_15h\n",
    "raw_data['kp_18h'] = kp_18h\n",
    "raw_data['kp_21h'] = kp_21h\n",
    "\n",
    "kp_predict = pd.DataFrame(raw_data)\n",
    "\n",
    "kp_predict.to_csv('./kp_predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
