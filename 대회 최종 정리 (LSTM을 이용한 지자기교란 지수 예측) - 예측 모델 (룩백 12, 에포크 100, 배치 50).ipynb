{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 데이터 읽어오기 (변수 5개 (Np, Vp, Bz, Bt, Bl)만 남긴 combine)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "path='C:\\Projects\\keras_talk\\comp_model_data\\\\'\n",
    "\n",
    "combine_data = pd.read_csv(path + 'combine_interpolation_an.csv', engine='python') \n",
    "\n",
    "kp_data = pd.read_csv(path + 'kp_data.csv') # 지자기교란 지수 파일에서 엑셀 자체로 index부분 없애고 kp_data로 이름 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, result_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(0, len(signal_data), look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back)]) # look_back = 12 단위로 3시간씩 끊어짐 - [0:12],[12:24],...\n",
    "    for j in range(0, len(result_data)):\n",
    "        for k in range(0, 8): # 24시간이 3시간 단위로 쪼개져서 총 8개의 column 있음\n",
    "            dataY.append(result_data[j][k]) # kp지수 데이터의 하나 하나를 뽑아내어 3시간의 combine 데이터와 대응시킴\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "signal_data = combine_data.values\n",
    "result_data = kp_data.values\n",
    "\n",
    "x_train = signal_data[0:385728] # 2920*12*8 + 2928*12*3 (00,04,08년 윤년) = 385728 (99~09년까지의 데이터를 train으로)\n",
    "x_val = signal_data[385728:455808] # 2920*12*2=70080 (10~11년까지의 데이터를 val으로)\n",
    "x_test = signal_data[455808:] # 2920*12*1 + 2928*12*1 =70080 (12년 윤년) (12~13년까지의 데이터를 test으로)\n",
    "\n",
    "y_train = result_data[0:4018] # 99~09년도\n",
    "y_val = result_data[4018:4748] # 10~11년도\n",
    "y_test = result_data[4748:] # 12~13년도\n",
    "\n",
    "\n",
    "# 데이터셋 생성\n",
    "\n",
    "x_train, y_train = create_dataset(x_train, y_train, look_back)\n",
    "x_val, y_val = create_dataset(x_val, y_val, look_back)\n",
    "x_test, y_test = create_dataset(x_test, y_test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 (RNN은 3차원으로 처리를 해야 한다고 함)\n",
    "# 변수 5개 (Np, Vp, Bz, Bt, Bl)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 5) # (number, timestep, feature)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 5)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 (어떤 값을 리스트 내에 0,1로 표시하는 벡터로 표현하도록 함, 예를 들어 3인 경우엔 [0,0,0,1,0,0,0,0,0,0]으로 표현됨)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "size = 10\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=size)\n",
    "y_val = to_categorical(y_val, num_classes=size)\n",
    "y_test = to_categorical(y_test, num_classes=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 구성하기\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "Optimizer = Adam(lr = 0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(48,input_shape=(12,5)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=100, batch_size=50, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:  [0.06547767443479502, 0.454641612742658]\n",
      "Validataion Score:  [0.06438208816394414, 0.46472602739726027]\n",
      "Test Score:  [0.06219835975366643, 0.49042407652584863]\n"
     ]
    }
   ],
   "source": [
    "# 5. 모델 평가하기\n",
    "\n",
    "trainScore = model.evaluate(x_train, y_train, verbose=0)\n",
    "model.reset_states()\n",
    "print('Train Score: ', trainScore)\n",
    "valScore = model.evaluate(x_val, y_val, verbose=0)\n",
    "model.reset_states()\n",
    "print('Validataion Score: ', valScore)\n",
    "testScore = model.evaluate(x_test, y_test, verbose=0)\n",
    "model.reset_states()\n",
    "print('Test Score: ', testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모델 사용하기 (final_result_prob_another도 interpolation 후 변수 5개만 뽑아서 problem_interpolation1로 만듦)\n",
    "\n",
    "path='C:\\Projects\\keras_talk\\comp_model_data\\\\'\n",
    "\n",
    "xhat = pd.read_csv(path + 'problem_interpolation1.csv', engine='python')\n",
    "\n",
    "input_data = xhat.values\n",
    "\n",
    "#input_data = input_data.reshape(input_data.shape[0], input_data.shape[1], 1)\n",
    "\n",
    "#input_data = input_data.reshape(2920, 12, 5) - 이 두 줄이 필요한지 안한지는 실행해봐야 알 것 같음.\n",
    "\n",
    "forecast_result = model.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 kp지수를 엑셀로 다시 정리\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(0,2920):\n",
    "    result.append(np.argmax(forecast_result[i]))\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for i in range(0, len(result), 8):\n",
    "    final_result.append(result[i:i+8])\n",
    "\n",
    "kp_0h = []\n",
    "kp_3h = []\n",
    "kp_6h = []\n",
    "kp_9h = []\n",
    "kp_12h = []\n",
    "kp_15h = []\n",
    "kp_18h = []\n",
    "kp_21h = []\n",
    "\n",
    "for i in range(0, len(final_result)):\n",
    "    kp_0h.append(final_result[i][0])\n",
    "    kp_3h.append(final_result[i][1])\n",
    "    kp_6h.append(final_result[i][2])\n",
    "    kp_9h.append(final_result[i][3])\n",
    "    kp_12h.append(final_result[i][4])\n",
    "    kp_15h.append(final_result[i][5])\n",
    "    kp_18h.append(final_result[i][6])\n",
    "    kp_21h.append(final_result[i][7])\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "raw_data = OrderedDict()\n",
    "\n",
    "raw_data['kp_0h'] = kp_0h\n",
    "raw_data['kp_3h'] = kp_3h\n",
    "raw_data['kp_6h'] = kp_6h\n",
    "raw_data['kp_9h'] = kp_9h\n",
    "raw_data['kp_12h'] = kp_12h\n",
    "raw_data['kp_15h'] = kp_15h\n",
    "raw_data['kp_18h'] = kp_18h\n",
    "raw_data['kp_21h'] = kp_21h\n",
    "\n",
    "kp_predict = pd.DataFrame(raw_data)\n",
    "\n",
    "kp_predict.to_csv('./kp_predict.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
